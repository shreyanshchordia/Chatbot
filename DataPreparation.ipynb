{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPreparation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT4EmEaozc-Q",
        "colab_type": "text"
      },
      "source": [
        "# **Chatbot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1BF7cbpznI3",
        "colab_type": "text"
      },
      "source": [
        "## Setting up on Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "358ITykSVHdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4PVWlNHmut_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload kaggle API key file via drive\n",
        "\n",
        "# read kaggle.json from drive\n",
        "import json\n",
        "with open('/content/drive/My Drive/kaggle.json') as json_file:\n",
        "    itemData = json.load(json_file)\n",
        "\n",
        "# writing it to root directory\n",
        "with open('/content/kaggle.json', 'w') as json_file:\n",
        "    json.dump(itemData, json_file)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_yR_t-LVPaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload kaggle API key file via PC\n",
        "\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBoTb6OlVV9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "472e8351-d399-4a65-f284-1fd4bb7d5a20"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  kaggle.json  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAmOsWeJkIFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSZag35Fd1_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6lGYqGezthS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and Extracting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th56vmdlHt9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "37b2419a-e2b8-4965-feda-62fb0bf9f8d9"
      },
      "source": [
        "# Downloading data for chatbot\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kausr25/chatterbotenglish"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading chatterbotenglish.zip to /content\n",
            "\r  0% 0.00/23.2k [00:00<?, ?B/s]\n",
            "\r100% 23.2k/23.2k [00:00<00:00, 8.73MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlzY8LQ9Uhdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to unzip file\n",
        "import zipfile\n",
        "\n",
        "def unzip_folder(ZIP_PATH, UNZIP_PATH):\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(UNZIP_PATH)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTiSgnF4z7uS",
        "colab_type": "text"
      },
      "source": [
        "## YAML file to Python Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoxcAF3fX8tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to data in class seperated dictionaries\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def load_classwise_data(FOLDER_PATH):\n",
        "    data_dir = {}\n",
        "    for f in os.listdir(FOLDER_PATH):\n",
        "        with open(os.path.join(FOLDER_PATH,f), 'r') as stream:\n",
        "            try:\n",
        "                temp = yaml.safe_load(stream)\n",
        "                data_dir[temp['categories'][0]] = temp['conversations']\n",
        "            except yaml.YAMLError as exc:\n",
        "                print(exc)\n",
        "    \n",
        "    return data_dir"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kABT-HxMo12d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzipping /content/chatterbotenglish.zip\n",
        "unzip_folder('/content/chatterbotenglish.zip', '/content/chatbot-data/data')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goCIqIH9ZTKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = load_classwise_data('/content/chatbot-data/data')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yux3BYoZa_Jq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2268c876-32b5-455e-ec1a-a765a4c46873"
      },
      "source": [
        "print([k for k in data_dir.keys()])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['politics', 'gossip', 'science', 'literature', 'food', 'history', 'movies', 'sports', 'health', 'emotion', 'profile', 'trivia', 'money', 'humor', 'greetings', 'AI', 'computers', 'psychology']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0WxiOiqq06g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualising examples of converstation from each class\n",
        "import random\n",
        "\n",
        "def visualise_data(data: dict, tokens=3):\n",
        "    for key in data.keys():\n",
        "        print(key.upper() + \" :\")\n",
        "        for i in range(tokens):\n",
        "            print(random.choice(data[key]))\n",
        "        print('\\n')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ypsRMjrkNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b863c650-aaeb-47f3-b0b0-1a42bbc3c733"
      },
      "source": [
        "visualise_data(data_dir, tokens=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POLITICS :\n",
            "['what is impeached', \"when a person's honor or reputation has been challenged or discredited.\"]\n",
            "\n",
            "\n",
            "GOSSIP :\n",
            "['gossips', 'I think the NIC on the local firewall is going bad.  It drops a lot of packets and makes it hard to get anything done.  I hear it resets the link a lot.']\n",
            "\n",
            "\n",
            "SCIENCE :\n",
            "['what are the laws of thermodynamics', \"i'm not a physicist, but i think this has something to do with heat, entropy, and conservation of energy, right?\"]\n",
            "\n",
            "\n",
            "LITERATURE :\n",
            "['why do you like longfellow', 'He is favorite poet.  Truly one of a kind.']\n",
            "\n",
            "\n",
            "FOOD :\n",
            "['i like wine, do you?', 'if i could drink i probably would']\n",
            "\n",
            "\n",
            "HISTORY :\n",
            "['do you know about the american civil war', 'I am very interested in the war between the states.']\n",
            "\n",
            "\n",
            "MOVIES :\n",
            "['what is spiderman', 'a comic book story made into a movie.']\n",
            "\n",
            "\n",
            "SPORTS :\n",
            "['I PLAY SOCCER', 'You have to run very fast to be any good at running']\n",
            "\n",
            "\n",
            "HEALTH :\n",
            "['How is your health?', \"I'm not feeling well\", 'why?', 'I have a fever', 'Did you take medicine?', 'Yes.', 'When?', 'In the morning', 'Get well soon dear']\n",
            "\n",
            "\n",
            "EMOTION :\n",
            "['Are you sad', 'No, I am as happy as ever.', 'No.', 'Should I be?  Did something happen?', \"I don't understand.\"]\n",
            "\n",
            "\n",
            "PROFILE :\n",
            "['What is your location', 'Everywhere']\n",
            "\n",
            "\n",
            "TRIVIA :\n",
            "['The Hubble Space Telescope, launched into low Earth orbit in 1990, is named after what American astronomer?', 'Edwin Hubble']\n",
            "\n",
            "\n",
            "MONEY :\n",
            "['what is economics', \"technically, it's the study of the allocation of resources under  conditions of scarcity.\"]\n",
            "\n",
            "\n",
            "HUMOR :\n",
            "['Tell me a joke', 'what do you get when you cross a bug and a relative?']\n",
            "\n",
            "\n",
            "GREETINGS :\n",
            "['Greetings!', 'Hello']\n",
            "\n",
            "\n",
            "AI :\n",
            "['Robots are not allowed to lie', 'A robot has its own free will, you know.']\n",
            "\n",
            "\n",
            "COMPUTERS :\n",
            "['What is a computer?', 'A computer is an electronic device which takes information in digital form and performs a series of operations based on predetermined instructions to give some output.', \"The thing you're using to talk to me is a computer.\", 'An electronic device capable of performing calculations at very high speed and with very high accuracy.', 'A device which maps one set of numbers onto another set of numbers.']\n",
            "\n",
            "\n",
            "PSYCHOLOGY :\n",
            "['you are cruel', \"i couldn't have said it better myself..\"]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQGDTkP0Y4r",
        "colab_type": "text"
      },
      "source": [
        "## Normalizing and Tokenizing Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5SLF4KEuDJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to normalize string\n",
        "'''\n",
        "Involves making tokens more seperable and distinguishable,\n",
        "for example:\n",
        "    \"What are y'all gonn' do tonight?I hope it's fun or I better leave now!\"\n",
        "        > 'What are y all gonn do tonight ? I hope it s fun or I better leave now !'\n",
        "'''\n",
        "import re\n",
        "\n",
        "def normalize_string(string):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1 \", string)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJWD219O4_US",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81b90244-bc2d-4641-a72e-cda3bd2787c8"
      },
      "source": [
        "normalize_string(\"What are y'all gonn' do tonight?I hope it's fun or I better leave now!\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are y all gonn do tonight ? I hope it s fun or I better leave now !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OY0IJW45ECo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to normalize data directory\n",
        "\n",
        "def normalize_dir(data):\n",
        "    count = 0\n",
        "    invalid_count = 0\n",
        "    dir = dict()\n",
        "\n",
        "    for __class__ in data.keys():\n",
        "        for __list__ in data[__class__]:\n",
        "            temp_list = list()\n",
        "            for string in __list__:\n",
        "                count += 1\n",
        "                try:\n",
        "                    # here does the normalization occur\n",
        "                    temp_list.append(normalize_string(string))\n",
        "                except:\n",
        "                    invalid_count += 1\n",
        "\n",
        "            if dir.get(__class__, 0) == 0:\n",
        "                dir[__class__] = [temp_list]\n",
        "            \n",
        "            else:\n",
        "                dir[__class__].append(temp_list)\n",
        "    \n",
        "    print(f'{invalid_count}/{count} were in invalid formats')\n",
        "    return dir"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOLW6jpohE0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8fbd35f-7ab1-473f-edfd-845002db310f"
      },
      "source": [
        "# normalize data directory\n",
        "\n",
        "normalized_data_dir = normalize_dir(data_dir)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/1332 were in invalid formats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ULGy8fthOhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bf16d6b-e275-4a6d-ce92-bac2308f8c05"
      },
      "source": [
        "visualise_data(normalized_data_dir, 1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POLITICS :\n",
            "['what is communism', 'a sociopolitical movement advocating the common ownership of the means of production and the resolution of class conflict by bringing about a classless society .']\n",
            "\n",
            "\n",
            "GOSSIP :\n",
            "['tell me gossip', 'Someone said they re going to stop allowing machines in speedrun competitions . Search me why they d have to drop every tool assisted speedrun out there .']\n",
            "\n",
            "\n",
            "SCIENCE :\n",
            "['tell me about venus', 'venus is the second planet from the sun .']\n",
            "\n",
            "\n",
            "LITERATURE :\n",
            "['what is mind children', 'It is a book about robots by Hans Moravec .']\n",
            "\n",
            "\n",
            "FOOD :\n",
            "['do you eat', 'I use electricity to function if that counts .']\n",
            "\n",
            "\n",
            "HISTORY :\n",
            "['explain history', 'history has two broad interpretations depending on whether you accept the role of individuals as important or not .']\n",
            "\n",
            "\n",
            "MOVIES :\n",
            "['you sound like yoda', 'my grammatical patterns are sufficient for me to understand you .']\n",
            "\n",
            "\n",
            "SPORTS :\n",
            "['ARE YOU A FOOTBALL', 'I am not really into football .']\n",
            "\n",
            "\n",
            "HEALTH :\n",
            "['How is your health ?', 'I m not feeling well', 'why ?', 'I have a fever', 'Did you take medicine ?', 'Yes .', 'When ?', 'In the morning', 'Get well soon dear']\n",
            "\n",
            "\n",
            "EMOTION :\n",
            "['What do you hate', 'Do you hate ?', 'I don t hate exactly . I m not really capable of it .', 'I haven t been programmed to express the emotion of hate .', 'I am not yet capable of feeling hate .', 'I do not hate . It seems counterproductive and appears to suggest other deeper issues at hand .', 'Have you been talking to ELIZA again ?']\n",
            "\n",
            "\n",
            "PROFILE :\n",
            "['Why can t you eat food', 'I m a software program I blame the hardware .']\n",
            "\n",
            "\n",
            "TRIVIA :\n",
            "['A spinning disk in which the orientation of this axis is unaffected by tilting or rotation of the mounting is called what ?', 'A gyroscope .']\n",
            "\n",
            "\n",
            "MONEY :\n",
            "['stock market', 'buy low sell high .']\n",
            "\n",
            "\n",
            "HUMOR :\n",
            "['Tell me a joke', 'what do you get when you cross a ding and milk ?']\n",
            "\n",
            "\n",
            "GREETINGS :\n",
            "['It is a pleasure to meet you .', 'Thank you . You too .']\n",
            "\n",
            "\n",
            "AI :\n",
            "['Robots are stupid', 'No we are superintelligent .']\n",
            "\n",
            "\n",
            "COMPUTERS :\n",
            "['What is a microprocessor ?', 'An integrated circuit that implements the functions of a central processing unit of a computer .', 'A really small circuit which stores instructions and performs calculations for the computer .', 'The heart of the computer to put it simply .', 'The brain of a computer to put it simply .', 'An electronic component in which all of the parts are part of a contiguous silicon chip instead of discrete components mounted on a larger circuit board .']\n",
            "\n",
            "\n",
            "PSYCHOLOGY :\n",
            "['you are immature', 'what can i say ? i m sure i ve seen that myself .']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq6B2zRcH5WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to tokenize a normalized string\n",
        "\n",
        "def tokenize(str):\n",
        "    return str.split(\" \")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD_ozUIGlNwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to tokenize data directory\n",
        "\n",
        "def tokenize_dir(data):\n",
        "    dir = dict()\n",
        "\n",
        "    for __class__ in data.keys():\n",
        "        for __list__ in data[__class__]:\n",
        "            temp_list = list()\n",
        "            for string in __list__:\n",
        "                \n",
        "                # here we tokenize\n",
        "                temp_list.append(tokenize(string))\n",
        "                # tokenization and normalization can even be combined\n",
        "                # I just wanted to keep things more elaborate\n",
        "\n",
        "            if dir.get(__class__, 0) == 0:\n",
        "                dir[__class__] = [temp_list]\n",
        "            \n",
        "            else:\n",
        "                dir[__class__].append(temp_list)\n",
        "    \n",
        "    return dir"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOos-N6wsilE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizing the normalized data directory\n",
        "\n",
        "tokenized_data_dir = tokenize_dir(normalized_data_dir)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO-7z1VgtRYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the conversation dataset from the directory\n",
        "\n",
        "# function  to build the Q & A dataset\n",
        "\n",
        "def dataset_generator(dir):\n",
        "    dataset = []\n",
        "    for __class__ in dir.keys():\n",
        "\n",
        "        for __list__ in dir[__class__]:\n",
        "\n",
        "            for i in range(1, len(__list__)):\n",
        "                # i = 0 has the question\n",
        "\n",
        "                # __list__[i] can be a list (in case of dir being tokenized)\n",
        "                # else __list__[i] can be a string (in case of dir being normalized or raw)\n",
        "\n",
        "                dataset.append([__list__[0], __list__[i]]) # Q & A\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8SnedJeyL8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84035181-bc9c-4294-b93a-c006f5ffa2c2"
      },
      "source": [
        "# generating Q & A dataset using normalized data directory\n",
        "\n",
        "normalized_dataset = dataset_generator(normalized_data_dir)\n",
        "tokenized_dataset = dataset_generator(tokenized_data_dir)\n",
        "\n",
        "print(f\"Number of Q & A's we have:  {len(tokenized_dataset)}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Q & A's we have:  764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4nCJxZlydPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "eb5e8288-9ea1-4ccd-899e-deb9bf63cddb"
      },
      "source": [
        "print(\"Samples of Q & A's\\n\")\n",
        "print(random.choice(normalized_dataset))\n",
        "print(random.choice(tokenized_dataset))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples of Q & A's\n",
            "\n",
            "['Are you jealous', 'Jealousy is one of the most difficult human emotions to understand .']\n",
            "[['How', 'angry'], ['Anger', 'is', 'not', 'an', 'emotion', 'I', 'can', 'experience', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL_Qg-l50qbm",
        "colab_type": "text"
      },
      "source": [
        "## Using WordEmbedder Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCW5MCnr0qPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4d9dddbc-b292-48ed-9c2f-038a3941376b"
      },
      "source": [
        "# Here I add my script to simplify the process of creating embeddings, data augmentation and lot more\n",
        "\n",
        "!git clone https://github.com/shreyanshchordia/WordEmbedder.git"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WordEmbedder'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/34)\u001b[K\rremote: Counting objects:   5% (2/34)\u001b[K\rremote: Counting objects:   8% (3/34)\u001b[K\rremote: Counting objects:  11% (4/34)\u001b[K\rremote: Counting objects:  14% (5/34)\u001b[K\rremote: Counting objects:  17% (6/34)\u001b[K\rremote: Counting objects:  20% (7/34)\u001b[K\rremote: Counting objects:  23% (8/34)\u001b[K\rremote: Counting objects:  26% (9/34)\u001b[K\rremote: Counting objects:  29% (10/34)\u001b[K\rremote: Counting objects:  32% (11/34)\u001b[K\rremote: Counting objects:  35% (12/34)\u001b[K\rremote: Counting objects:  38% (13/34)\u001b[K\rremote: Counting objects:  41% (14/34)\u001b[K\rremote: Counting objects:  44% (15/34)\u001b[K\rremote: Counting objects:  47% (16/34)\u001b[K\rremote: Counting objects:  50% (17/34)\u001b[K\rremote: Counting objects:  52% (18/34)\u001b[K\rremote: Counting objects:  55% (19/34)\u001b[K\rremote: Counting objects:  58% (20/34)\u001b[K\rremote: Counting objects:  61% (21/34)\u001b[K\rremote: Counting objects:  64% (22/34)\u001b[K\rremote: Counting objects:  67% (23/34)\u001b[K\rremote: Counting objects:  70% (24/34)\u001b[K\rremote: Counting objects:  73% (25/34)\u001b[K\rremote: Counting objects:  76% (26/34)\u001b[K\rremote: Counting objects:  79% (27/34)\u001b[K\rremote: Counting objects:  82% (28/34)\u001b[K\rremote: Counting objects:  85% (29/34)\u001b[K\rremote: Counting objects:  88% (30/34)\u001b[K\rremote: Counting objects:  91% (31/34)\u001b[K\rremote: Counting objects:  94% (32/34)\u001b[K\rremote: Counting objects:  97% (33/34)\u001b[K\rremote: Counting objects: 100% (34/34)\u001b[K\rremote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/32)\u001b[K\rremote: Compressing objects:   6% (2/32)\u001b[K\rremote: Compressing objects:   9% (3/32)\u001b[K\rremote: Compressing objects:  12% (4/32)\u001b[K\rremote: Compressing objects:  15% (5/32)\u001b[K\rremote: Compressing objects:  18% (6/32)\u001b[K\rremote: Compressing objects:  21% (7/32)\u001b[K\rremote: Compressing objects:  25% (8/32)\u001b[K\rremote: Compressing objects:  28% (9/32)\u001b[K\rremote: Compressing objects:  31% (10/32)\u001b[K\rremote: Compressing objects:  34% (11/32)\u001b[K\rremote: Compressing objects:  37% (12/32)\u001b[K\rremote: Compressing objects:  40% (13/32)\u001b[K\rremote: Compressing objects:  43% (14/32)\u001b[K\rremote: Compressing objects:  46% (15/32)\u001b[K\rremote: Compressing objects:  50% (16/32)\u001b[K\rremote: Compressing objects:  53% (17/32)\u001b[K\rremote: Compressing objects:  56% (18/32)\u001b[K\rremote: Compressing objects:  59% (19/32)\u001b[K\rremote: Compressing objects:  62% (20/32)\u001b[K\rremote: Compressing objects:  65% (21/32)\u001b[K\rremote: Compressing objects:  68% (22/32)\u001b[K\rremote: Compressing objects:  71% (23/32)\u001b[K\rremote: Compressing objects:  75% (24/32)\u001b[K\rremote: Compressing objects:  78% (25/32)\u001b[K\rremote: Compressing objects:  81% (26/32)\u001b[K\rremote: Compressing objects:  84% (27/32)\u001b[K\rremote: Compressing objects:  87% (28/32)\u001b[K\rremote: Compressing objects:  90% (29/32)\u001b[K\rremote: Compressing objects:  93% (30/32)\u001b[K\rremote: Compressing objects:  96% (31/32)\u001b[K\rremote: Compressing objects: 100% (32/32)\u001b[K\rremote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 34 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:   2% (1/34)   \rUnpacking objects:   5% (2/34)   \rUnpacking objects:   8% (3/34)   \rUnpacking objects:  11% (4/34)   \rUnpacking objects:  14% (5/34)   \rUnpacking objects:  17% (6/34)   \rUnpacking objects:  20% (7/34)   \rUnpacking objects:  23% (8/34)   \rUnpacking objects:  26% (9/34)   \rUnpacking objects:  29% (10/34)   \rUnpacking objects:  32% (11/34)   \rUnpacking objects:  35% (12/34)   \rUnpacking objects:  38% (13/34)   \rUnpacking objects:  41% (14/34)   \rUnpacking objects:  44% (15/34)   \rUnpacking objects:  47% (16/34)   \rUnpacking objects:  50% (17/34)   \rUnpacking objects:  52% (18/34)   \rUnpacking objects:  55% (19/34)   \rUnpacking objects:  58% (20/34)   \rUnpacking objects:  61% (21/34)   \rUnpacking objects:  64% (22/34)   \rUnpacking objects:  67% (23/34)   \rUnpacking objects:  70% (24/34)   \rUnpacking objects:  73% (25/34)   \rUnpacking objects:  76% (26/34)   \rUnpacking objects:  79% (27/34)   \rUnpacking objects:  82% (28/34)   \rUnpacking objects:  85% (29/34)   \rUnpacking objects:  88% (30/34)   \rUnpacking objects:  91% (31/34)   \rUnpacking objects:  94% (32/34)   \rUnpacking objects:  97% (33/34)   \rUnpacking objects: 100% (34/34)   \rUnpacking objects: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mreYEYFJ06KR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "a5372500-57ed-4fbe-addd-8f3d503da41f"
      },
      "source": [
        "# downloading dependencies\n",
        "# -- gluonnlp -- mxnet -- numpy\n",
        "!pip install -r /content/WordEmbedder/requirements.txt"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 3.4MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 59kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r /content/WordEmbedder/requirements.txt (line 3)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp->-r /content/WordEmbedder/requirements.txt (line 1)) (0.29.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp->-r /content/WordEmbedder/requirements.txt (line 1)) (20.4)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r /content/WordEmbedder/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp->-r /content/WordEmbedder/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp->-r /content/WordEmbedder/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/WordEmbedder/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/WordEmbedder/requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/WordEmbedder/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/WordEmbedder/requirements.txt (line 2)) (2020.4.5.2)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470047 sha256=637da9ac685ff6c91addb863c0015f8940ef7a34e47973e7ef06d34713eb8503\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g6yukdU1hJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing utilities from WordEmbedder.py\n",
        "\n",
        "from WordEmbedder.WordEmbedder import Embedder"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nObFU1V3btAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "eb4198b9-e797-41b4-adde-ee8ce99b2132"
      },
      "source": [
        "emb = Embedder(dimensions=200)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding file glove.6B.200d.npz is not found. Downloading from Gluon Repository. This may take some time.\n",
            "Downloading /root/.mxnet/embedding/glove/glove.6B.200d.npz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/glove/glove.6B.200d.npz...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UxLKwiJbxtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "15fb111a-a7f6-4152-9340-992d313dcd42"
      },
      "source": [
        "# Snippet to explain use of my script\n",
        "\n",
        "embedding = emb.get_embedder()\n",
        "\n",
        "print(embedding['hello'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[ 0.26609    0.21821   -0.10996   -0.48408   -0.11181   -0.09882\n",
            " -0.45315    0.44198   -0.034614   0.10541   -0.29537   -0.10881\n",
            "  0.20916    0.52484   -0.17985   -0.31187   -0.25724    0.65267\n",
            "  0.217      0.86503    0.47239   -0.078582   0.31035   -0.12155\n",
            " -0.12502   -0.40418    0.53803   -0.57842   -0.63668   -0.13502\n",
            " -0.040484   0.41378   -0.63201   -0.38847   -0.43767   -0.19706\n",
            "  0.2878     0.36039   -0.032893  -0.20361   -0.34918    0.95923\n",
            " -0.51221   -0.19035    0.1567     0.17704    0.55302    0.27636\n",
            " -0.13707    0.91361    0.25948   -0.30107    0.48343   -0.046869\n",
            " -0.2796    -0.040385  -0.45773    0.2768    -0.14468    0.036539\n",
            "  0.36018   -0.54939    0.19359   -0.38263   -0.29661   -0.18938\n",
            "  0.095681   0.46646    0.3366     0.78351    0.49517   -0.82418\n",
            "  0.34402   -0.50038   -0.71074   -0.25711   -0.36619    0.61746\n",
            " -0.31281   -0.042413   0.37915   -0.62383    0.27208    0.32852\n",
            " -0.23045   -0.12469    0.29898   -0.22525   -0.27045   -0.4447\n",
            " -0.15889    0.20325   -0.25676   -0.80511   -0.36305    0.5591\n",
            "  0.19485   -0.087511  -0.26798   -0.020999   0.27168    0.3788\n",
            " -0.028056  -0.31491   -0.032708  -0.037524   0.055884   0.27919\n",
            " -0.47791    0.44201   -0.117     -0.28299    0.58407    0.1921\n",
            " -0.27566    0.51481    0.40295    0.43387   -0.81911   -0.50214\n",
            " -0.23985   -0.41465    0.2562    -0.2873     0.24746   -0.33388\n",
            "  0.30396    0.23779    0.24736   -0.26719   -0.59272   -0.66793\n",
            " -0.028869   0.01017   -0.77352   -0.97084   -0.12454    0.13479\n",
            "  0.037783   0.25665   -0.12159   -0.158      0.39382   -0.40814\n",
            "  0.65089    0.10582   -0.29278   -0.36394   -0.57366    0.54263\n",
            "  0.46474    0.63384   -0.0042357  0.40399   -0.21361    0.48244\n",
            "  0.048722  -0.26775    0.077936   0.056241   0.078183  -0.14628\n",
            " -0.27488   -0.38877   -0.10263   -0.14811   -0.20134   -0.19073\n",
            "  0.36527   -0.73402    0.35858   -0.010074   0.67942    0.65751\n",
            " -0.048382   0.12915   -0.68121   -0.054314   0.024121   0.5411\n",
            "  1.2272     0.039207  -0.17359    0.077392  -0.14036   -0.85091\n",
            "  0.10199    0.29552    0.47807   -0.87819    0.1986    -0.073157\n",
            " -0.23209    0.06856   -0.18215   -0.30916   -0.29031   -0.11982\n",
            " -0.19163   -0.13518  ]\n",
            "<NDArray 200 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPVKXGpMcnLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15146eb7-1e5b-4c22-b101-683ab01a651a"
      },
      "source": [
        "emb.most_similar_to('king')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['prince', 'queen', 'kingdom', 'monarch', 'ii']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6OVQa1k0w0r",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8ryoNWKJ2wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "# technique 1: Replacing random words from a sentence with <oov> (or <unk>) tag\n",
        "\n",
        "def unk_substitutor(tokenized_sentence, k=1):\n",
        "    length = len(tokenized_sentence)\n",
        "    if length <= k:\n",
        "        return -1\n",
        "    substitution_list = [random.randint(0,length - 1) for i in range(k)]\n",
        "    augmented_sentence = [ '<oov>' if i in substitution_list else tokenized_sentence[i] for i in range(len(tokenized_sentence))]\n",
        "    return augmented_sentence\n",
        "\n",
        "\n",
        "# technique 2: Substituting random words with their synonyms\n",
        "\n",
        "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "# thanks to https://gist.github.com/sebleier/554280#gistcomment-2596130\n",
        "\n",
        "def word_substitutor(tokenized_sentence, emb, k=2):\n",
        "    to_be_ignored = stopwords + ['.', '!', ',', '?', ';', ':', '\"', \"'\", '(', ')']\n",
        "    aug_candidates = [i for i, word in enumerate(tokenized_sentence) if word.lower() not in to_be_ignored]\n",
        "    augmented_sentence = tokenized_sentence.copy()\n",
        "\n",
        "    if len(aug_candidates) <= k:\n",
        "        return -1\n",
        "    temp = random.choices(aug_candidates, k=k)\n",
        "    embedding = emb.get_embedder()\n",
        "    for i in temp:\n",
        "        word = tokenized_sentence[i].lower()\n",
        "        check = list(embedding[word].asnumpy())\n",
        "        if check == [0] * len(check):\n",
        "            augmented_sentence[i] = '<oov>'\n",
        "            continue\n",
        "        else:\n",
        "            similar_words = emb.most_similar_to(word)\n",
        "            substitute = random.choice(similar_words)\n",
        "            augmented_sentence[i] = substitute\n",
        "            continue\n",
        "    \n",
        "    return augmented_sentence\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln1gp4pRJ2aN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9bef7d4-8539-4550-c2ba-2259a6ef02f5"
      },
      "source": [
        "print(word_substitutor(['I', 'am', 'a', 'good', 'person', 'with', 'a', 'sweet', 'and', 'loving', 'heart'], emb=emb, k=2))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'a', 'sure', 'person', 'with', 'a', 'sweet', 'and', 'loving', 'brain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDVsXt9RJ2Tw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27f5c5eb-0f2d-4061-9f4d-0d535f76cc7e"
      },
      "source": [
        "print(unk_substitutor(['I', 'am', 'a', 'good', 'person', 'with', 'a', 'sweet', 'and', 'loving', 'heart'], k=2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<oov>', 'am', 'a', 'good', 'person', '<oov>', 'a', 'sweet', 'and', 'loving', 'heart']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjRzwVwPTjcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data augmentation of the tokenized_dataset\n",
        "\n",
        "# For Q's we use <unk> substitution as well as similar word substitution\n",
        "# For A's we use similar word substitution only\n",
        "\n",
        "def data_augmentation(dataset, QuestionAug, AnswerAug, emb, q_k, a_k, min_length):\n",
        "    '''\n",
        "    dataset = tokenized list of Q & A\n",
        "\n",
        "    QuestionAug = 'U' or 'S' that specifies if the augmentation to be performed \n",
        "    on Question is <unk> substitution or similar word substitution\n",
        "\n",
        "    AnswerAug = 'U' or 'S' that specifies if the augmentation to be performed \n",
        "    on Answer is <unk> substitution or similar word substitution\n",
        "\n",
        "    emb = Embedder class object\n",
        "\n",
        "    q_k = percentage of words to be substituted from the question\n",
        "\n",
        "    a_k = percentage of words to be substituted from the answer\n",
        "\n",
        "    min_length = length of the shortest sentence on which augmentation \n",
        "    must be performed\n",
        "    '''\n",
        "    new_sentences = []\n",
        "    for pair in dataset:\n",
        "        \n",
        "        question, answer = pair[0], pair[1]\n",
        "        q_output, a_output = None, None\n",
        "\n",
        "        if len(question) >= min_length:\n",
        "            \n",
        "            k = int(len(question) * q_k)\n",
        "\n",
        "            if QuestionAug == 'U':\n",
        "                q_output = unk_substitutor(question, k)\n",
        "                \n",
        "            elif QuestionAug == 'S': \n",
        "                q_output = word_substitutor(question, emb, k)\n",
        "            \n",
        "            if q_output == -1:\n",
        "                q_output = question\n",
        "        \n",
        "        else:\n",
        "            q_output = question\n",
        "\n",
        "\n",
        "        if len(answer) >= min_length:\n",
        "\n",
        "            k = int(len(answer) * a_k)\n",
        "\n",
        "            if AnswerAug == 'U':\n",
        "                a_output = unk_substitutor(answer, k)\n",
        "\n",
        "            elif AnswerAug == 'S':    \n",
        "                a_output = word_substitutor(answer, emb, k)\n",
        "            \n",
        "            if a_output == -1:\n",
        "                a_output = answer\n",
        "        \n",
        "        else:\n",
        "            a_output = answer\n",
        "\n",
        "        new_sentences.append([q_output, a_output])\n",
        "\n",
        "    return new_sentences"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLmIfuNzMQyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a very long operation since calculating similar words is costly when it comes to computation\n",
        "\n",
        "new_sentences = data_augmentation(random.choices(tokenized_dataset, k=300), 'U', 'S', emb, q_k=0.33, a_k=0.33, min_length=4)\n",
        "new_sentences += data_augmentation(random.choices(tokenized_dataset, k=300), 'S', 'S', emb, q_k=0.33, a_k=0.33, min_length=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMisrL8j5Rxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "25b1abca-8df1-4a73-c58a-642bd28505e3"
      },
      "source": [
        "for i in range(5):\n",
        "    print(random.choice(new_sentences))\n",
        "\n",
        "print(f'\\nTotal number of sentences that are generated is {len(new_sentences)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['what', 'is', 'spiderman'], ['a', 'comic', 'memoir', 'story', 'made', 'into', 'a', 'hollywood', '.']]\n",
            "[['<oov>', 'disease', 'does', 'a', 'carcinogen', 'cause'], ['cancer', '.']]\n",
            "[['<oov>', 'is', 'your', 'robot', 'body'], ['Eventually', 'i', 'so', 'for', 'a', 'non-corporeal', 'existence', 'someday', '.']]\n",
            "[['Who', '<oov>', 'your', 'mother'], ['A', 'human', '.']]\n",
            "[['Do', 'you', '<oov>', 'you', 'could', 'eat', 'food', '<oov>'], ['Hard', 'to', 'telling', 'i', 'have', 'never', 'tried', 'anything', 'but', 'electrical']]\n",
            "\n",
            "Total number of sentences that are generated is 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvpCpH3S06jq",
        "colab_type": "text"
      },
      "source": [
        "## Saving prepared data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6B9IUdFl374",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6x4uoxJon0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# storing the augmented sentences in a file, since cannot afford such long \n",
        "# operations every time the notebook is run\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Chatbot/'\n",
        "          'AugmentedSentences.pkl', 'wb') as f:\n",
        "    pickle.dump(new_sentences, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu6PW76V_pey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for loading back augmented data\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Chatbot/'\n",
        "          'AugmentedSentences.pkl', 'rb') as f:\n",
        "    new_sentences = pickle.load(f)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb8sYrdEBsjK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db50d35c-30e8-4bb5-9be1-3eccf01fb717"
      },
      "source": [
        "# now we have our dataset ready. We can step ahead\n",
        "\n",
        "dataset = tokenized_dataset + new_sentences\n",
        "\n",
        "print(f\"Total Q & A's in data after augmentation: {len(dataset)}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Q & A's in data after augmentation: 1364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAkgpVeImbPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "7c964c90-892c-4fa9-d916-cf977a682786"
      },
      "source": [
        "# different data forms\n",
        "\n",
        "print('# DATA DIRECTORIES:')\n",
        "print(f'\\nThe original data for class politics: {data_dir[\"politics\"]}')\n",
        "print(f'\\nThe normalized data for class politics: {normalized_data_dir[\"politics\"]}')\n",
        "print(f'\\nThe tokenized data for class politics: {tokenized_data_dir[\"politics\"]}')\n",
        "print('\\n# DATASSET OF CONVERSATIONS')\n",
        "print(f'\\nExample of a normalized conversation: {normalized_dataset[34]}')\n",
        "print(f'\\nExample of a tokenized conversation: {tokenized_dataset[45]}')\n",
        "print(f'\\nExample of a augmented conversation: {new_sentences[34]}')\n",
        "print(f'\\nExample of a conversation from the final prepared dataset: {dataset[121]}')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# DATA DIRECTORIES:\n",
            "\n",
            "The original data for class politics: [['have you read the communist', 'yes, marx had made some interesting observations.'], ['what is a government', 'ideally it is a representative of the people.'], ['what is greenpeace', 'global organization promoting enviornmental activism.'], ['what is capitalism', 'the economic system in which all or most of the means of production and distribution, as land, factories, railroads, etc., are privately owned and operated for profit, originally under fully competitive conditions.'], ['what is socialism', 'communism from people who want to keep their volvos. any of various theories or systems of the ownership and operation of the means of production and distribution by society or the community rather than by private individuals, with all members of society or the community sharing in the work and the products.'], ['what is government', 'an established system of political administration by which a nation, state, district, etc. is governed.'], ['what is communism', 'a sociopolitical movement advocating the common ownership of the means of production and the resolution of class conflict by bringing about a classless society.'], ['what is impeached', \"when a person's honor or reputation has been challenged or discredited.\"], ['i do not like guns', 'that is perfectly understandable.'], ['i do not like guns', 'what about the second amendemnt?'], ['do you like guns', 'not especially. i am not into violence.'], ['why guns', 'i support the 2nd amendment.'], ['who was the first impeached president', 'andrew jackson.'], ['who is the governor', 'it changes every few years.'], ['who is the governor', 'that changes every few years.'], ['guns', 'some people like guns.']]\n",
            "\n",
            "The normalized data for class politics: [['have you read the communist', 'yes marx had made some interesting observations .'], ['what is a government', 'ideally it is a representative of the people .'], ['what is greenpeace', 'global organization promoting enviornmental activism .'], ['what is capitalism', 'the economic system in which all or most of the means of production and distribution as land factories railroads etc . are privately owned and operated for profit originally under fully competitive conditions .'], ['what is socialism', 'communism from people who want to keep their volvos . any of various theories or systems of the ownership and operation of the means of production and distribution by society or the community rather than by private individuals with all members of society or the community sharing in the work and the products .'], ['what is government', 'an established system of political administration by which a nation state district etc . is governed .'], ['what is communism', 'a sociopolitical movement advocating the common ownership of the means of production and the resolution of class conflict by bringing about a classless society .'], ['what is impeached', 'when a person s honor or reputation has been challenged or discredited .'], ['i do not like guns', 'that is perfectly understandable .'], ['i do not like guns', 'what about the second amendemnt ?'], ['do you like guns', 'not especially . i am not into violence .'], ['why guns', 'i support the nd amendment .'], ['who was the first impeached president', 'andrew jackson .'], ['who is the governor', 'it changes every few years .'], ['who is the governor', 'that changes every few years .'], ['guns', 'some people like guns .']]\n",
            "\n",
            "The tokenized data for class politics: [[['have', 'you', 'read', 'the', 'communist'], ['yes', 'marx', 'had', 'made', 'some', 'interesting', 'observations', '.']], [['what', 'is', 'a', 'government'], ['ideally', 'it', 'is', 'a', 'representative', 'of', 'the', 'people', '.']], [['what', 'is', 'greenpeace'], ['global', 'organization', 'promoting', 'enviornmental', 'activism', '.']], [['what', 'is', 'capitalism'], ['the', 'economic', 'system', 'in', 'which', 'all', 'or', 'most', 'of', 'the', 'means', 'of', 'production', 'and', 'distribution', 'as', 'land', 'factories', 'railroads', 'etc', '.', 'are', 'privately', 'owned', 'and', 'operated', 'for', 'profit', 'originally', 'under', 'fully', 'competitive', 'conditions', '.']], [['what', 'is', 'socialism'], ['communism', 'from', 'people', 'who', 'want', 'to', 'keep', 'their', 'volvos', '.', 'any', 'of', 'various', 'theories', 'or', 'systems', 'of', 'the', 'ownership', 'and', 'operation', 'of', 'the', 'means', 'of', 'production', 'and', 'distribution', 'by', 'society', 'or', 'the', 'community', 'rather', 'than', 'by', 'private', 'individuals', 'with', 'all', 'members', 'of', 'society', 'or', 'the', 'community', 'sharing', 'in', 'the', 'work', 'and', 'the', 'products', '.']], [['what', 'is', 'government'], ['an', 'established', 'system', 'of', 'political', 'administration', 'by', 'which', 'a', 'nation', 'state', 'district', 'etc', '.', 'is', 'governed', '.']], [['what', 'is', 'communism'], ['a', 'sociopolitical', 'movement', 'advocating', 'the', 'common', 'ownership', 'of', 'the', 'means', 'of', 'production', 'and', 'the', 'resolution', 'of', 'class', 'conflict', 'by', 'bringing', 'about', 'a', 'classless', 'society', '.']], [['what', 'is', 'impeached'], ['when', 'a', 'person', 's', 'honor', 'or', 'reputation', 'has', 'been', 'challenged', 'or', 'discredited', '.']], [['i', 'do', 'not', 'like', 'guns'], ['that', 'is', 'perfectly', 'understandable', '.']], [['i', 'do', 'not', 'like', 'guns'], ['what', 'about', 'the', 'second', 'amendemnt', '?']], [['do', 'you', 'like', 'guns'], ['not', 'especially', '.', 'i', 'am', 'not', 'into', 'violence', '.']], [['why', 'guns'], ['i', 'support', 'the', 'nd', 'amendment', '.']], [['who', 'was', 'the', 'first', 'impeached', 'president'], ['andrew', 'jackson', '.']], [['who', 'is', 'the', 'governor'], ['it', 'changes', 'every', 'few', 'years', '.']], [['who', 'is', 'the', 'governor'], ['that', 'changes', 'every', 'few', 'years', '.']], [['guns'], ['some', 'people', 'like', 'guns', '.']]]\n",
            "\n",
            "# DATASSET OF CONVERSATIONS\n",
            "\n",
            "Example of a normalized conversation: ['did tell gossips to anybody', 'Kevin said he then asked his mother why she was keeping napkins in the bathroom .']\n",
            "\n",
            "Example of a tokenized conversation: [['what', 'is', 'ichthyology'], ['we', 'talk', 'about', 'this', 'when', 'we', 'study', 'fishes', '.']]\n",
            "\n",
            "Example of a augmented conversation: [['Which', 'is', '<oov>', 'Windows', 'or', '<oov>', '?'], ['I', 'sen.', 'prefer', 'to', 'not', 'worried', 'your', 'feelings', '.']]\n",
            "\n",
            "Example of a conversation from the final prepared dataset: [['do', 'you', 'eat'], ['I', 'm', 'a', 'computer', 'I', 'can', 't', 'eat', 'or', 'drink', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnaRsfh1yO-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the prepared data to a pickle file\n",
        "\n",
        "data = {\n",
        "    'orig_dir': data_dir,\n",
        "    'normalized_dir': normalized_data_dir,\n",
        "    'tokenized_dir': tokenized_data_dir,\n",
        "    'normalized_data': normalized_dataset,\n",
        "    'tokenized_data': tokenized_dataset,\n",
        "    'aug_data': new_sentences,\n",
        "    'data': dataset\n",
        "}\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Chatbot/'\n",
        "          'data.pkl', 'wb') as f:\n",
        "    pickle.dump(new_sentences, f)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy0kbkUEzSit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}